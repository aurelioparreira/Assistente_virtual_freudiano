# -*- coding: utf-8 -*-
"""Assistente virtual freudiano

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GIjCX939e6TkeUlrxpVBAUQexHe9Eyh6
"""

!pip -q install -U pip
!pip -q install -U "click>=8.2.1"
!pip -q install pdfplumber openai
!pip -q install git+https://github.com/openai/whisper.git
!pip -q install edge-tts
!apt-get -y install ffmpeg

"""# 1. Montar o Google Drive para acessar os PDFs üìÇ"""

from google.colab import drive
drive.mount('/content/drive')

"""
# 2. Localizar e filtrar PDFs das obras de Freud üìñ"""

import os, re
import pdfplumber

pdf_path = "/content/drive/MyDrive/"  # seus PDFs do Freud est√£o na raiz do MyDrive, conforme sua listagem

# Lista PDFs na pasta
all_pdfs = [f for f in os.listdir(pdf_path) if f.lower().endswith(".pdf")]

# Filtro por nome (ajuste as palavras-chave se quiser)
pattern = r"freud|obras completas|imago|histeria|sonhos|psicanal"
freud_pdfs = [f for f in all_pdfs if re.search(pattern, f.lower())]

print("\nQtd PDFs na pasta:", len(all_pdfs))
print("Qtd PDFs do Freud (filtrados):", len(freud_pdfs))
print("Exemplos (at√© 10):", freud_pdfs[:10])

if len(freud_pdfs) == 0:
    raise FileNotFoundError(
        "N√£o encontrei PDFs do Freud com o filtro atual. "
        "Ajuste o 'pattern' ou mova seus PDFs para uma pasta espec√≠fica e atualize pdf_path."
    )

"""# 3. Extrair texto dos PDFs (corpus)"""

corpus_parts = []
total_pages = 0
pages_with_text = 0

# Ajuste para testes: limite de p√°ginas (None = tudo)
MAX_PAGES = 200  # ex.: 200 para teste

for fname in sorted(freud_pdfs):
    full = os.path.join(pdf_path, fname)
    try:
        with pdfplumber.open(full) as pdf:
            for page in pdf.pages:
                total_pages += 1
                text = page.extract_text()
                if text and text.strip():
                    pages_with_text += 1
                    corpus_parts.append(text)

                if MAX_PAGES is not None and total_pages >= MAX_PAGES:
                    break
    except Exception as e:
        print(f"[AVISO] Falha ao ler {fname}: {e}")

    if MAX_PAGES is not None and total_pages >= MAX_PAGES:
        break

corpus = "\n".join(corpus_parts)

print("\n--- Corpus ---")
print("P√°ginas lidas:", total_pages)
print("P√°ginas com texto extra√≠do:", pages_with_text)
print("Tamanho do corpus (chars):", len(corpus))

if pages_with_text == 0:
    print("\n[ATEN√á√ÉO] Nenhuma p√°gina retornou texto com pdfplumber.")
    print("Isso normalmente significa PDF escaneado (imagem). Nesse caso voc√™ precisar√° OCR (pytesseract).")

"""# 4. Grava√ß√£o de √°udio üé§"""

from IPython.display import HTML, display
from google.colab import output
from base64 import b64decode
import uuid, os

_last_audio_path = None

def _save_audio_js(data_url, out_webm="/content/request_audio.webm"):
    global _last_audio_path
    audio_bytes = b64decode(data_url.split(",")[1])
    with open(out_webm, "wb") as f:
        f.write(audio_bytes)
    _last_audio_path = out_webm
    return "OK"

cb_name = f"rec_{uuid.uuid4().hex}"
output.register_callback(cb_name, _save_audio_js)

display(HTML(f"""
<div style="display:flex; gap:12px; align-items:center;">
  <button id="startBtn" style="padding:10px 14px;">üéôÔ∏è Gravar</button>
  <button id="stopBtn" style="padding:10px 14px;" disabled>‚èπÔ∏è Parar</button>
  <span id="status" style="font-family:monospace;">Pronto.</span>
</div>

<script>
let stream, recorder, chunks;

const startBtn = document.getElementById("startBtn");
const stopBtn  = document.getElementById("stopBtn");
const statusEl = document.getElementById("status");

function blobToDataURL(blob) {{
  return new Promise((resolve) => {{
    const reader = new FileReader();
    reader.onloadend = () => resolve(reader.result);
    reader.readAsDataURL(blob);
  }});
}}

startBtn.onclick = async () => {{
  try {{
    statusEl.textContent = "Pedindo permiss√£o do microfone...";
    stream = await navigator.mediaDevices.getUserMedia({{ audio: true }});
    recorder = new MediaRecorder(stream);
    chunks = [];

    recorder.ondataavailable = e => chunks.push(e.data);
    recorder.onstart = () => {{
      statusEl.textContent = "Gravando... (clique em Parar)";
      startBtn.disabled = true;
      stopBtn.disabled = false;
    }};
    recorder.onstop = async () => {{
      statusEl.textContent = "Processando √°udio...";
      const blob = new Blob(chunks, {{ type: recorder.mimeType }});
      const dataUrl = await blobToDataURL(blob);

      await google.colab.kernel.invokeFunction("{cb_name}", [dataUrl], {{}});
      statusEl.textContent = "√Åudio enviado. Rode a pr√≥xima c√©lula.";

      stopBtn.disabled = true;
      startBtn.disabled = false;
      stream.getTracks().forEach(t => t.stop());
    }};

    recorder.start();
  }} catch (e) {{
    statusEl.textContent = "Erro ao acessar microfone: " + e;
    startBtn.disabled = false;
    stopBtn.disabled = true;
  }}
}};

stopBtn.onclick = () => {{
  if (recorder && recorder.state === "recording") recorder.stop();
}};
</script>
"""))

"""# 5. Transcri√ß√£o com Whisper üß†"""

import whisper
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print("\nGPU dispon√≠vel?", torch.cuda.is_available(), "| device:", device)

model = whisper.load_model("small").to(device)

result = model.transcribe(
    record_file,
    fp16=(device == "cuda"),
    language="pt"
)

transcription = result["text"].strip()
print("\nTranscri√ß√£o:", transcription)

"""# 6. OpenAI (SDK atual) + resposta üí¨"""

import os
os.environ["OPENAI_API_KEY"] = "SUA CHAVE AQUI"
import os
from openai import OpenAI
import re

# üîê Pegar chave da vari√°vel de ambiente
api_key = os.environ.get("OPENAI_API_KEY", "").strip()

# ‚ùó Verifica√ß√£o de seguran√ßa
if not api_key:
    raise ValueError(
        "OPENAI_API_KEY n√£o encontrada.\n"
        "Defina a vari√°vel com:\n"
        "os.environ['OPENAI_API_KEY'] = 'SUA_CHAVE'\n"
        "ou use o Secrets do Colab."
    )

# üîë Criar cliente
client = OpenAI(api_key=api_key)

# Sele√ß√£o simples de contexto (heur√≠stica) para n√£o mandar corpus aleat√≥rio
def pick_context_simple(corpus_text: str, query: str, max_chars=6000):
    words = re.findall(r"\w+", query.lower())
    if not words:
        return corpus_text[:2000]

    hits = []
    current_len = 0

    # usa at√© 12 palavras mais informativas (aproxima√ß√£o simples)
    key_words = [w for w in words if len(w) >= 4][:12]

    for line in corpus_text.split("\n"):
        low = line.lower()
        score = sum(1 for w in key_words if w in low)
        if score >= 2:
            hits.append(line)
            current_len += len(line) + 1
            if current_len >= max_chars:
                break

    if hits:
        return "\n".join(hits)[:max_chars]

    return corpus_text[:2000]

contexto = pick_context_simple(corpus, transcription, max_chars=6000)

resp = client.responses.create(
    model="gpt-4.1-mini",
    input=[
        {
            "role": "system",
            "content": (
                "Voc√™ √© um analista virtual inspirado em Freud. "
                "Use conceitos psicanal√≠ticos freudianos e linguagem clara, √∫til e cuidadosa. "
                "Fa√ßa perguntas complementares quando necess√°rio. "
                "N√£o invente cita√ß√µes com p√°ginas; use refer√™ncias gerais ao corpus quando fizer sentido."
            ),
        },
        {
            "role": "user",
            "content": (
                f"Pergunta (transcri√ß√£o do √°udio): {transcription}\n\n"
                f"Trechos do corpus freudiano (para apoio):\n{contexto}"
            ),
        },
    ],
)

chatgpt_response = resp.output_text.strip()
print("\nResposta:\n", chatgpt_response)

"""# 6. S√≠ntese de voz com edge-TTS üîä"""

import edge_tts
from IPython.display import Audio, display

async def tts_edge(text, out_mp3="/content/response_audio.mp3", voice="pt-BR-FranciscaNeural"):
    communicate = edge_tts.Communicate(text=text, voice=voice)
    await communicate.save(out_mp3)
    return out_mp3

response_audio = await tts_edge(chatgpt_response)  # ‚úÖ aqui √© await, n√£o asyncio.run
display(Audio(response_audio, autoplay=True))
